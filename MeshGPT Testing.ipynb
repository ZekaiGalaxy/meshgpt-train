{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-19T01:39:38.155749Z","iopub.status.busy":"2023-12-19T01:39:38.155103Z","iopub.status.idle":"2023-12-19T01:39:44.048426Z","shell.execute_reply":"2023-12-19T01:39:44.047605Z","shell.execute_reply.started":"2023-12-19T01:39:38.155692Z"},"trusted":true},"outputs":[],"source":["import torch\n","import trimesh\n","import numpy as np\n","import os\n","import csv \n","import json\n","import math \n","from collections import OrderedDict\n","\n","from meshgpt_pytorch import (\n","    MeshTransformerTrainer,\n","    MeshAutoencoderTrainer,\n","    MeshAutoencoder,\n","    MeshTransformer\n",")\n","\n","def get_3d_data(file_path): \n","    mesh = trimesh.load(file_path, force='mesh')\n","    \n","    # Extract vertices and faces\n","    vertices = mesh.vertices.tolist()\n","    faces = mesh.faces.tolist()\n","    centered_vertices = vertices - np.mean(vertices, axis=0)\n"," \n","    max_abs = np.max(np.abs(centered_vertices))\n","    vertices = centered_vertices / (max_abs / 0.95)  \n","      \n","      \n","    def sort_vertices(vertex): # Sort by Y , X, Z.  Y is vertical\n","        return vertex[1], vertex[0], vertex[2]   \n"," \n","    seen = OrderedDict()\n","    for point in vertices: \n","      key = tuple(point)\n","      if key not in seen:\n","        seen[key] = point\n","    unique_vertices =  list(seen.values()) \n","    sorted_vertices = sorted(unique_vertices, key=sort_vertices)\n","     \n","    vertices_as_tuples = [tuple(v) for v in vertices]\n","    sorted_vertices_as_tuples = [tuple(v) for v in sorted_vertices]\n","\n","    vertex_map = {old_index: new_index for old_index, vertex_tuple in enumerate(vertices_as_tuples) for new_index, sorted_vertex_tuple in enumerate(sorted_vertices_as_tuples) if vertex_tuple == sorted_vertex_tuple}\n"," \n","\n","    reindexed_faces = [[vertex_map[face[0]], vertex_map[face[1]], vertex_map[face[2]]] for face in faces] \n","    return np.array(sorted_vertices), np.array(reindexed_faces)\n","\n","def augment_mesh_scalar(vertices, scale_factor):\n","    # Apply a scalar factor to XYZ coordinates\n","    transformed_vertices = vertices * scale_factor\n","    return transformed_vertices\n","\n","def generate_scale_factors(num_examples, lower_limit=0.75, upper_limit=1.25): \n","    scale_factors = np.random.uniform(lower_limit, upper_limit, size=num_examples)\n","    return scale_factors\n","\n","def jitter_mesh(vertices, jitter_factor=0.01): \n","    offsets = np.random.uniform(-jitter_factor, jitter_factor, size=vertices.shape)\n"," \n","    jittered_vertices = vertices + offsets \n","    return jittered_vertices \n","\n","def augment_mesh(vertices, scale_factor):\n","    #vertices = jitter_mesh(vertices)\n","    transformed_vertices = vertices * scale_factor\n","    \n","    return transformed_vertices\n"," \n","\n","def load_models(directory, num_examples, variations):\n","    obj_datas = []  \n","    \n","    print(f\"num_examples: {num_examples}\")\n","    for filename in os.listdir(directory):  \n","        if (filename.endswith(\".obj\") or  filename.endswith(\".glb\") or  filename.endswith(\".off\")):\n","            file_path = os.path.join(directory, filename)\n","\n","            scale_factors = generate_scale_factors(variations, 0.7, 0.9) \n","            vertices, faces = get_3d_data(file_path) \n","\n","            for scale_factor in scale_factors: \n","                aug_vertices = augment_mesh(vertices.copy(), scale_factor) \n","                \n","                for _ in range(num_examples):\n","                    obj_data = {\"vertices\": aug_vertices.tolist(), \"faces\":  faces.tolist(), \"texts\": filename[:-4]}\n","                    obj_datas.append(obj_data)   \n","    return obj_datas\n","  \n","\n","\n","  \n","def load_json(file,num_examples):\n","    obj_datas = []\n","    with open(file, \"r\") as json_file:\n","        loaded_data = json.load(json_file) \n","        for item in loaded_data:\n","            for _ in range(num_examples): \n","                obj_data = {\"vertices\": torch.tensor(item[\"vertices\"], dtype=torch.float).to(\"cuda\"), \"faces\":  torch.tensor(item[\"faces\"], dtype=torch.long).to(\"cuda\"),\"texts\": item[\"texts\"] } \n","                obj_datas.append(obj_data)\n","    return obj_datas\n","                        \n","         "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-19T01:39:44.050017Z","iopub.status.busy":"2023-12-19T01:39:44.049685Z","iopub.status.idle":"2023-12-19T01:39:44.063671Z","shell.execute_reply":"2023-12-19T01:39:44.062641Z","shell.execute_reply.started":"2023-12-19T01:39:44.049992Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch import Tensor, tensor\n","from torch.utils.data import Dataset, DataLoader \n","from tqdm import tqdm\n","import numpy as np \n","import gc\n","from torch.nn.utils.rnn import pad_sequence\n","from meshgpt_pytorch.data import ( \n","    derive_face_edges_from_faces\n",") \n"," \n","class MeshDataset(Dataset): \n","    \n","    def __init__(self, data): \n","        self.data = data\n","        print(f\"Got {len(data)} data\")\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx): \n","        data = self.data[idx] \n","        return data  \n","    \n","    def save(self, path):\n","        np.savez_compressed(path, *self.data)\n","\n","    \n","    @classmethod\n","    def load(cls, path): \n","        loaded_data = np.load(path, allow_pickle=True)\n"," \n","        data = []\n","        for i in range(len(loaded_data.files)):\n","            data_item = {}\n","            for key in loaded_data[f\"arr_{i}\"].item():\n","                data_item[key] = loaded_data[f\"arr_{i}\"].item()[key]\n","            data.append(data_item)\n","\n","        return cls(data)\n","    \n","    def embed_texts(self,transformer): \n","        unique_texts = set(item['texts'] for item in self.data)\n"," \n","        text_embeddings = transformer.embed_texts(list(unique_texts))\n","        print(f\"Got text_embeddings: {len(text_embeddings)}\") \n","        text_embedding_dict = dict(zip(unique_texts, text_embeddings))\n"," \n","        for item in self.data:\n","            text_value = item['texts']\n","            item['text_embeds'] = text_embedding_dict.get(text_value, None)\n","            del item['texts']\n","            \n","    def generate_face_edges(self):\n","        n = 0\n","        for i in range(0, len(self.data)):  \n","            item = self.data[i]\n","            item['face_edges'] =  derive_face_edges_from_faces(item['faces'])\n","            n += 1  \n","        print(f\"done {n}/{len(self.data)}\")\n","\n","    def generate_codes(self, autoencoder : MeshAutoencoder):\n","        n = 0\n","        for i in range(0, len(self.data)):  \n","            item = self.data[i]\n","             \n","            codes = autoencoder.tokenize(\n","                vertices = item['vertices'],\n","                faces = item['faces'],\n","                face_edges = item['face_edges']\n","            ) \n","            item['codes'] = codes \n","            n += 1  \n","\n","        print(f\"[generate_codes] done {n}/{len(self.data)}\") \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-19T01:39:44.066146Z","iopub.status.busy":"2023-12-19T01:39:44.065812Z","iopub.status.idle":"2023-12-19T01:39:52.313349Z","shell.execute_reply":"2023-12-19T01:39:52.312381Z","shell.execute_reply.started":"2023-12-19T01:39:44.066116Z"},"trusted":true},"outputs":[],"source":["import json\n","#tables = load_models(r\" filtered\",5,5)  \n","#with open(\"data.json\", \"w\") as json_file:\n","#    json.dump(tables, json_file) \n","\n","tables = load_json(\"/f_ndata/zekai/ShapeNetCore.v2/table100.json\",2)\n","dataset = MeshDataset(tables) \n","dataset.generate_face_edges()\n","dataset.data[0].keys()\n","\n","desired_order = ['vertices', 'faces', 'face_edges', 'texts']\n","\n","dataset.data = [\n","    {key: d[key] for key in desired_order} for d in dataset.data\n","]\n","\n","unique_values = set(item[\"texts\"] for item in dataset.data)\n","\n","print(len(unique_values))  \n","print(unique_values)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["autoencoder = MeshAutoencoder( \n","    dim = 512,\n","    use_residual_lfq = True,\n","    commit_loss_weight = 0.1,\n","    bin_smooth_blur_sigma = 0.4\n",")   \n","total_params = sum(p.numel() for p in autoencoder.encoders.parameters())\n","print(f\"encoders Total parameters: {total_params}\")\n","total_params = sum(p.numel() for p in autoencoder.decoders.parameters())\n","print(f\"decoders Total parameters: {total_params}\")  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-19T01:44:00.715656Z","iopub.status.busy":"2023-12-19T01:44:00.714727Z","iopub.status.idle":"2023-12-19T12:00:40.840982Z","shell.execute_reply":"2023-12-19T12:00:40.840110Z","shell.execute_reply.started":"2023-12-19T01:44:00.715620Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n","total_params = sum(p.numel() for p in autoencoder.encoders.parameters())\n","print(f\"Total parameters: {total_params}\")\n","print(autoencoder.encoders)\n","\n","autoencoder_trainer = MeshAutoencoderTrainer(model =autoencoder,learning_rate = 1e-3, \n","                                             warmup_steps = 10,\n","                                             dataset = dataset,   \n","                                             num_train_steps=100,\n","                                             batch_size=16,\n","                                             grad_accum_every=1)\n","\n","loss = autoencoder_trainer.train(40,stop_at_loss = 0.25)   \n","autoencoder_trainer = MeshAutoencoderTrainer(model =autoencoder,learning_rate = 1e-4, \n","                                             warmup_steps = 10,\n","                                             dataset = dataset,\n","                                             checkpoint_every_epoch = 20,  \n","                                             num_train_steps=100,\n","                                             batch_size=16,\n","                                             grad_accum_every=1)\n","\n","loss = autoencoder_trainer.train(180,stop_at_loss = 0.25)   \n","autoencoder_trainer.save(f'./mesh-encoder_2_loss_{loss:.3f}.pt') "]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["768\n","4608\n","Total parameters: 108722063\n"]}],"source":["max_length =  max(len(d[\"faces\"]) for d in dataset if \"faces\" in d) \n","max_seq =  max_length * 6  \n","print(max_length)\n","print(max_seq)\n","transformer = MeshTransformer(\n","    autoencoder,\n","    dim = 512,\n","    max_seq_len = max_seq,\n","    coarse_pre_gateloop_depth = 6,\n","    fine_pre_gateloop_depth= 4, \n","    condition_on_text = False\n",")\n","total_params = sum(p.numel() for p in transformer.parameters())\n","print(f\"Total parameters: {total_params}\") "]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/80: 100%|██████████| 200/200 [01:01<00:00,  3.27it/s, loss=9.49]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1 average loss: 9.711674628257752\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/80: 100%|██████████| 200/200 [01:00<00:00,  3.30it/s, loss=9.65]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2 average loss: 9.515366473197936\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/80: 100%|██████████| 200/200 [00:59<00:00,  3.37it/s, loss=9.21]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3 average loss: 9.3349600982666\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/80: 100%|██████████| 200/200 [01:00<00:00,  3.33it/s, loss=9.32]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4 average loss: 9.088741512298585\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/80:  86%|████████▌ | 172/200 [00:51<00:08,  3.31it/s, loss=8.09]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/root/meshgpt-train/MeshGPT Testing.ipynb 单元格 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504149227d/root/meshgpt-train/MeshGPT%20Testing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m MeshTransformerTrainer(model \u001b[39m=\u001b[39m transformer,warmup_steps \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m,grad_accum_every\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,num_train_steps\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,   dataset \u001b[39m=\u001b[39m dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504149227d/root/meshgpt-train/MeshGPT%20Testing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m                                  learning_rate \u001b[39m=\u001b[39m \u001b[39m1e-1\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504149227d/root/meshgpt-train/MeshGPT%20Testing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(\u001b[39m80\u001b[39;49m,stop_at_loss \u001b[39m=\u001b[39;49m \u001b[39m0.00009\u001b[39;49m)   \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504149227d/root/meshgpt-train/MeshGPT%20Testing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m trainer \u001b[39m=\u001b[39m MeshTransformerTrainer(model \u001b[39m=\u001b[39m transformer,warmup_steps \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m,grad_accum_every\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,num_train_steps\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,  dataset \u001b[39m=\u001b[39m dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504149227d/root/meshgpt-train/MeshGPT%20Testing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m                                  learning_rate \u001b[39m=\u001b[39m \u001b[39m1e-2\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504149227d/root/meshgpt-train/MeshGPT%20Testing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain(\u001b[39m80\u001b[39m,stop_at_loss \u001b[39m=\u001b[39m \u001b[39m0.00009\u001b[39m)    \n","File \u001b[0;32m~/meshgpt-train/meshgpt_pytorch/trainer.py:751\u001b[0m, in \u001b[0;36mMeshTransformerTrainer.train\u001b[0;34m(self, num_epochs, stop_at_loss, diplay_graph)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mautocast():\n\u001b[1;32m    750\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_kwargs)\n\u001b[0;32m--> 751\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss \u001b[39m/\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrad_accum_every)\n\u001b[1;32m    753\u001b[0m \u001b[39mif\u001b[39;00m exists(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_grad_norm):\n\u001b[1;32m    754\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mclip_grad_norm_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_grad_norm)\n","File \u001b[0;32m/opt/conda/envs/mesh/lib/python3.10/site-packages/accelerate/accelerator.py:1905\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1903\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1904\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1905\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m/opt/conda/envs/mesh/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n","File \u001b[0;32m/opt/conda/envs/mesh/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"]}],"source":[" \n","trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=1,num_train_steps=100,   dataset = dataset,\n","                                 learning_rate = 1e-1, batch_size=2)\n","trainer.train(80,stop_at_loss = 0.00009)   \n","\n"," \n","trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=1,num_train_steps=100,  dataset = dataset,\n","                                 learning_rate = 1e-2, batch_size=2)\n","trainer.train(80,stop_at_loss = 0.00009)    \n","\n","trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=1,num_train_steps=100,  dataset = dataset,\n","                                 learning_rate = 1e-4, batch_size=2)\n","trainer.train(80,stop_at_loss = 0.00009)   \n","\n","trainer.save(f'./mesh-transformer_2_{loss:.3f}.pt')    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unique_values = set(item[\"texts\"] for item in dataset.data)\n","print(len(unique_values))  \n","coords = []\n","for text in unique_values: \n","    print(f\"doing {text}\")\n","    faces_coordinates = transformer.generate(texts = [text]) \n","    coords.append(faces_coordinates)\n","    tensor_data = faces_coordinates[0].cpu()\n","    \n","    numpy_data = tensor_data.numpy().reshape(-1, 3)\n","    \n","    obj_file_content = \"\"\n","    \n","    for vertex in numpy_data:\n","        obj_file_content += f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\"\n","\n","    for i in range(1, len(numpy_data), 3):\n","        obj_file_content += f\"f {i} {i + 1} {i + 2}\\n\"\n","\n","    # Save to a file\n","    obj_file_path = f'./tests/3d_output_{text}.obj'\n","    with open(obj_file_path, \"w\") as file:\n","        file.write(obj_file_content)\n","\n","    print(obj_file_path) \n","    \n","    \n","all_vertices = []\n","all_faces = []\n","vertex_offset = 0\n"," \n","translation_distance = 0.3  \n","\n","for r, faces_coordinates in enumerate(coords): \n","    tensor_data = faces_coordinates[0].cpu()\n","\n","    numpy_data = tensor_data.numpy().reshape(-1, 3)\n","\n","    # Translate the model to avoid overlapping\n","    numpy_data[:, 0] += translation_distance * (r / 0.2 - 1)  # Adjust X coordinate\n","\n","    # Accumulate vertices\n","    for vertex in numpy_data:\n","        all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n","\n","    # Accumulate faces with adjusted indices\n","    for i in range(1, len(numpy_data), 3):\n","        all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n","\n","    # Update the vertex offset for the next model\n","    vertex_offset += len(numpy_data)\n","\n","# Combine vertices and faces\n","obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n","\n","# Save to a single file\n","obj_file_path = f\"./tests/3d_models_all.obj\"\n","with open(obj_file_path, \"w\") as file:\n","    file.write(obj_file_content)\n","\n","print(obj_file_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","coords_all = []\n","for text in set(item[\"texts\"] for item in dataset.data): \n","    print(f\"Doing {text}\")\n","    coords = []\n","    for r in np.arange(0, 1.0, 0.1):\n","        faces_coordinates = transformer.generate(temperature=r, texts = [text]) \n","        coords.append(faces_coordinates)\n","    coords_all.append(coords)\n","    \n","    all_vertices = []\n","    all_faces = []\n","    vertex_offset = 0\n","\n","    # Translation distance for each model\n","    translation_distance = 0.3  # Adjust as needed\n","\n","    for r, faces_coordinates in enumerate(coords): \n","        tensor_data = faces_coordinates[0].cpu()\n","\n","        numpy_data = tensor_data.numpy().reshape(-1, 3)\n","\n","        # Translate the model to avoid overlapping\n","        numpy_data[:, 0] += translation_distance * (r / 0.2 - 1)  # Adjust X coordinate\n","\n","        # Accumulate vertices\n","        for vertex in numpy_data:\n","            all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n","\n","        # Accumulate faces with adjusted indices\n","        for i in range(1, len(numpy_data), 3):\n","            all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n","\n","        # Update the vertex offset for the next model\n","        vertex_offset += len(numpy_data)\n","\n","    # Combine vertices and faces\n","    obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n","\n","    # Save to a single file\n","    obj_file_path = f\"./results/3d_models_{text}_temps.obj\"\n","    with open(obj_file_path, \"w\") as file:\n","        file.write(obj_file_content)\n","\n","    print(obj_file_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def loadModels():\n","    autoencoder = MeshAutoencoder(\n","        dim = 576,\n","        encoder_depth = 6,\n","        decoder_depth = 6,\n","        num_discrete_coors = 128  ,\n","        local_attn_depth =0, \n","        \n","    )\n","    autoencoder_trainer = MeshAutoencoderTrainer(model = autoencoder,\n","                                    learning_rate = 1e-1, \n","                                                checkpoint_every_epoch= 5,\n","                                                warmup_steps = 10,\n","                                                dataset = dataset,  \n","                                                num_train_steps=100,\n","                                                batch_size=2,\n","                                                grad_accum_every=1)\n","\n","    autoencoder_trainer.load(r\"mesh-encoder_last.pt\")\n","    encoder = autoencoder_trainer.model\n","    max_length =  max(len(d[\"faces\"]) for d in dataset if \"faces\" in d) \n","    max_seq =  max_length * 6  \n","    \n","    transformer = MeshTransformer(\n","        autoencoder,\n","        dim = 768,\n","        max_seq_len = max_seq,\n","        condition_on_text = True)\n","     \n","    trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=1,num_train_steps=100, checkpoint_folder = r\"F:\\MachineLearning\\Mesh\\MeshGPT\\checkpoints\" , dataset = dataset,\n","                                    learning_rate = 1e-3, batch_size=2) \n","    trainer.load(r\"mesh-transformer.pt\")\n","    transformer = trainer.model\n","    return transformer, encoder\n","\n","#transformer, autoencoder =  loadModels() "]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4189032,"sourceId":7234116,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
